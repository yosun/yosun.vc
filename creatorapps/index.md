### introduction, ideology and methodology

i like to dream of transparent computing as a creation interface: i/o that is intrinsically intuitive (maximally un-intrusive) and also helps to naturally guide the user's creativity and productivity. 

i explore pieces of technology that just happen to work at the serendipitous moment I chance to discover them, and similar to a well-prepared kitchen in a Michelin starred restaurant, I have these ready as ingredients that I might use in a master-chef creation.* 

i like extending creator tool analogies that work - for example, the concept of Photoshop layers when applied to AR effects to age-old vertical inspector panels for large-screen-estate creator apps. Or, applying the easel concept to do img2img LCM mappings in realtime sculpting in XR.

a prevalent theme has been: turning "on paper" idea into a 3D animated scene - i've been working on this since pre-gen-AI days.

as explained in my [[expressive computing]] exposition, i'm a software soloist and below are apps i've built independently - albeit, strategically timed to go with innovation trends. 

### expected interfaces: more traditional creator apps 

Many years ago, when mobile augmented reality (AR) creator software was a new thing, I built a Photoshop meets face tracking iOS/Android AR app called [FaceShop (2017)](https://faceshop.io), where you can mix hybrid 3D assets x VFX layers with blend mode effects to create AR "filters," which I exhibited at Makerfaire and Augmented World Expo. 

A year or so later, partly due to a month long online format TechCrunch Disrupt Hackathon hack period, this became [faceStylr (2018)](https://facestylr.com) - same idea, but commerce oriented: a consumer-creator tool for iOS/Android AR try-on of makeup, earrings, hats etc. (It also helped me win my[ second TechCrunch Disrupt Hackathon Grand Prize](https://devpost.com/software/facestylr).)

While both of the above apps are focused on larger screens, such as desktop or tablet interfaces, I also made an app called [ActormaticAR (2022)](https://ActormaticAR.com) that lets you design faces "artbreeder style" to AR puppeteer as auto-magical actors (after winning SIGGRAPH Real-Time Live in 2020 for [DrawmaticAR](https://DrawmaticAR.com), I found myself naming many apps-maticAR).

More recently, partly due to yet another million dollar hackathon, [I spent a few weeks using Meta Horizon Desktop](https://devpost.com/software/grow-homes), and realized all the follies of a poorly designed creator tool built using Unity UI - and myopic upper management who did not understand the power of Unity's cross platform compile engine (allowing for the platform to be on not just Windows, but on iOS/Android/Mac/etc). This then segwayed into the AI3D Render Desktop/Tablet app, which allows you to use 3D to have frame by frame control of your AI generated video. 

AI3D Render App (2025) lets you use keyframes in a 3D environment to control video generation; "AI3D Render" process. We also integrate the expected AI3D suite in the image to 3D process. There is also an augmented reality version that lets you animate AI video straight from reality. 


### experimental HCI x AI: new kinds of creator apps 

My first AR app ever was [ClayAR](https://blog.clayar.com): augmented reality clay sculpting on an AR marker using [Qualcomm Augmented Reality in 2010](https://www.youtube.com/watch?v=UADHnhhbwAw), and also the world's first mobile pen tablet device [Galaxy Tab in 2011](https://www.youtube.com/watch?v=Is5Z_NHBKds). (But, tbh, a much more fun use of a pen is for [SolderAR](https://www.youtube.com/watch?v=IO9n8pUdb08) - one of several apps I presented at [Makerfaire 2013 that delighted Palmer Luckey](https://x.com/Yosun/status/336548209591717888). And, also in 2015, I lazarus'd it as [PencilSolder](https://www.youtube.com/watch?v=AlCQ-10biv4))

I spent some time getting stuck in the Kinect-Intel RealSense bubble.

I started experimenting with drawing on paper as a way to create 3D AR in 2018 - using dlib to re-map what I drew on paper on a face template to my own face. faced.io was as collection of face-related AR apps I made between 2016-2019; [Project SUR](https://SUR.faced.io) specifically involved mapping surfaces to the face. I called this the [faced.io Digitizer](https://www.youtube.com/watch?v=YwBh1l9H_PI), and also tried pitching [Mattel Innovation](https://www.youtube.com/watch?v=5sMdsiU5aU8) and doing the [sharpie challenge](https://x.com/Yosun/status/1065412344333791232) with it without actually drawing on my face. 

Inspired in part from a series of [AR experimental musical instruments](https://www.youtube.com/watch?v=7vxrU_NnfZc&list=PL9-GubSWKfOYq5ZPAOJxO6NP-c0z6A1QO) and [holoShatter](https://holoshatter.com), I also did an experimental creator-artistic-musical-interface called [AR.Brushed.Ink](https://www.youtube.com/watch?v=-6Bjo5_YUQk) which lets you a "segment-shatter" Chinese brush painting into paint daubs as musical notes that you can tap to "play" and "color tap" "back in place". For me, it was one of many projects, and I think Anita had kids to take care of, so sadly post Augmented World Expo 2018 neither of us had the bandwidth to keep exhibiting the project. :-\ 

In 2019, I continued my explorations "on paper" with [PlayGAMI](https://PlayGAMI.com) - an augmented reality "creativity platform" that did the AR coloring book trick but for visualizing an origami crane you can actually fold~! And also a series of [mini-games](https://www.youtube.com/watch?v=Lig3sELHIwM) with [main character](https://www.youtube.com/watch?v=gHEB6qUBuZU) being the virtual version of the same crane that you colored/designed on paper. 

In 2020, an app I made with a confusing name of [DrawmaticAR](https://DrawmaticAR.com), which turns what kids write on real paper into 3D AR animated stories (on the same piece of paper) won SIGGRAPH Real-Time Live. 

My first bona fide gen AI app was [earrings.ai](https://earrings.ai) in Dec 2022, mostly just utilizing SD 1.5 as a way for people to create and collect earrings. 

I continued experimenting with paper with the Napkinmatic series: turn your napkin sketch into an AI canvas - from OCR directions to clickable links to napkin sketch to app to napkin sketch to masterpiece painting to napkin sketch to 3D animated talking heads and beyond in [SIGGRAPH Real-Time Live 2023](https://youtu.be/JLxqiMCrdwE?t=4341) (6 minute live demo in front of a live audience of thousands at LA Convention Center). 

In 2024, mostly because the Apple Vision Pro did not have the HD camera that I had dreamed for in an AR headset, I tried non-paper for a bit - exploring virtual skeuomorphic interfaces in designing virtual humans using AI3D Desktop and subsequently AI3D Sculpt with easel to LCM to image to 3D, and AI3D Co-Create (AI3D Primitives). 

### Founder + Hacker in Residence at "AI3D Foundation" 

I started the AI3D Foundation noticing that there seems to be a gap between Gen AI 3D models and HCI that better allow humans to expressively interface with them. The agenda aligns with my general purpose to create more intuitive 3D creator tools, and so I'm continuing my experimentation as founder and hacker in residence. 

The AI3D Foundation is an open research software development project that produces timely, innovative and relevant results, at the intersection of breakthrough HCI and AI content generation, presenting regularly at top research conferences such as SIGGRAPH, CVPR, ECCV etc. The AI3D foundation is also building/maintaining a framework and platform for more easily accessing multimodal AI models (such as video and image to 3D models). 




 _*  i had my photogrammetry-augmented reality foodie adventures working with celebrity chefs to 3D scan their food for posterity with my HoloYummy project from 2012-2016) 
